% Evaluating a Learning Algorithm
% Termanteus
% 20 January 2020

\pagebreak
\tableofcontents
\pagebreak

# I. Evaluating a Hypothesis
- Someway to improve our algorithm:
  - Getting more training examples
  - Trying smaller sets of features
  - Trying additional features
  - Trying polynomial features
  - Increasing or decreasing Î»

After doing the mentioned solution, we can move on to evaluate our new hypothesis.

A hypothesis might have low error for training set but inaccurate in real situation (overfitting). Thus, to evaluate it, we divide the training set to 2 sets: a training set and a test set. Training set ~ 70% and the test set ~30%.

The new procedure using these two sets is:
1. Learn $\Theta$ and minimize $J_{train}(\Theta)$ using the training set
2. Compute the test set error $J_{test}(\Theta)$ 

## The test set error

1. For linear regression: $J_{test}(\Theta) = \frac{1}{2m_{test}}\sum_{i=1}^{m_{test}}(h_{\Theta}(x_{test}^{(i)}) - y_{test}^{(i)})^{2}$
2. For classification ~ Misclassification error (aka 0/1 misclassification error):

$$err(h_{\Theta}(x), y) = 1: if\ h_{\Theta}(x) >= 0.5\ and\ y=0\ or\ h_{\Theta}(x) < 0.5\ and\ y = 1$$
$$err(h_{\Theta}(x), y) = 0: otherwise $$

This gives us a binary 0 or 1 error result. The average test error for the test set is:
$$Test\_Error = \frac{1}{m_{test}}\sum_{i=1}^{m_{test}}err(h_{\Theta}(x_{test}^{(i)},y_{test}^{(i)}))$$
